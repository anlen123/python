{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from pandas import DataFrame\n",
    "from snownlp import SnowNLP\n",
    "from sklearn.model_selection import  train_test_split\n",
    "import jieba as jb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>PL</th>\n      <th>SnowNLP</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0</td>\n      <td>达芬奇的恶魔（一语双关）</td>\n      <td>0.70</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1</td>\n      <td>@战双帕弥什</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2</td>\n      <td>我狂掉达尔文[微笑][微笑][微笑]</td>\n      <td>0.85</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3</td>\n      <td>[战双帕弥什_诶嘿][战双帕弥什_诶嘿][战双帕弥什_诶嘿]</td>\n      <td>1.00</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>4</td>\n      <td>永动姬</td>\n      <td>0.47</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>39995</td>\n      <td>39995</td>\n      <td>喜迎芬芳一周年，一路相伴张大仙（燃向）av28566658</td>\n      <td>0.95</td>\n    </tr>\n    <tr>\n      <td>39996</td>\n      <td>39996</td>\n      <td>另外说一下，视频后面的寒假作业，是仙友自制的688村委会文件，而不是现实生活中的寒假作业啦～...</td>\n      <td>0.91</td>\n    </tr>\n    <tr>\n      <td>39997</td>\n      <td>39997</td>\n      <td>图片来源于微博https://m.weibo.cn/1791027450/432633668...</td>\n      <td>0.97</td>\n    </tr>\n    <tr>\n      <td>39998</td>\n      <td>39998</td>\n      <td>换碟前bgm是什么</td>\n      <td>0.47</td>\n    </tr>\n    <tr>\n      <td>39999</td>\n      <td>39999</td>\n      <td>我怀疑腾讯的员工都快过劳死，每天都在更新</td>\n      <td>0.01</td>\n    </tr>\n  </tbody>\n</table>\n<p>40000 rows × 3 columns</p>\n</div>",
      "text/plain": "       Unnamed: 0                                                 PL  SnowNLP\n0               0                                       达芬奇的恶魔（一语双关）     0.70\n1               1                                             @战双帕弥什     1.00\n2               2                                 我狂掉达尔文[微笑][微笑][微笑]     0.85\n3               3                     [战双帕弥什_诶嘿][战双帕弥什_诶嘿][战双帕弥什_诶嘿]     1.00\n4               4                                                永动姬     0.47\n...           ...                                                ...      ...\n39995       39995                      喜迎芬芳一周年，一路相伴张大仙（燃向）av28566658     0.95\n39996       39996  另外说一下，视频后面的寒假作业，是仙友自制的688村委会文件，而不是现实生活中的寒假作业啦～...     0.91\n39997       39997  图片来源于微博https://m.weibo.cn/1791027450/432633668...     0.97\n39998       39998                                          换碟前bgm是什么     0.47\n39999       39999                               我怀疑腾讯的员工都快过劳死，每天都在更新     0.01\n\n[40000 rows x 3 columns]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读入数据\n",
    "data = pd.read_excel(r'F:\\python\\代码\\B站\\情感分析\\评论表1.xlsx')\n",
    "# data = pd.read_csv(r'F:\\python\\代码\\B站\\情感分析\\pinlun_full.txt',sep='\\n')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'comment'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-76965e189356>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m#先对前100个数据进行打分\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'SnowNLP'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msnow_result\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ANACONDA\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5177\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5178\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5179\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5181\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'comment'"
     ]
    }
   ],
   "source": [
    "#调用SnowNLP对评论进行评分\n",
    "def snow_result(comemnt):\n",
    "    S = round(SnowNLP(comemnt).sentiments,2)\n",
    "    return S\n",
    "data = data[:100]  #先对前100个数据进行打分\n",
    "data['SnowNLP']=data.comment.apply(snow_result)\n",
    "print(0)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>PL</th>\n      <th>SnowNLP</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0</td>\n      <td>达芬奇的恶魔（一语双关）</td>\n      <td>0.70</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>1</td>\n      <td>@战双帕弥什</td>\n      <td>1.00</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>2</td>\n      <td>我狂掉达尔文[微笑][微笑][微笑]</td>\n      <td>0.85</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>3</td>\n      <td>[战双帕弥什_诶嘿][战双帕弥什_诶嘿][战双帕弥什_诶嘿]</td>\n      <td>1.00</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>4</td>\n      <td>永动姬</td>\n      <td>0.47</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>39995</td>\n      <td>39995</td>\n      <td>喜迎芬芳一周年，一路相伴张大仙（燃向）av28566658</td>\n      <td>0.95</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <td>39996</td>\n      <td>39996</td>\n      <td>另外说一下，视频后面的寒假作业，是仙友自制的688村委会文件，而不是现实生活中的寒假作业啦～...</td>\n      <td>0.91</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <td>39997</td>\n      <td>39997</td>\n      <td>图片来源于微博https://m.weibo.cn/1791027450/432633668...</td>\n      <td>0.97</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <td>39998</td>\n      <td>39998</td>\n      <td>换碟前bgm是什么</td>\n      <td>0.47</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <td>39999</td>\n      <td>39999</td>\n      <td>我怀疑腾讯的员工都快过劳死，每天都在更新</td>\n      <td>0.01</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>40000 rows × 4 columns</p>\n</div>",
      "text/plain": "       Unnamed: 0                                                 PL  SnowNLP  \\\n0               0                                       达芬奇的恶魔（一语双关）     0.70   \n1               1                                             @战双帕弥什     1.00   \n2               2                                 我狂掉达尔文[微笑][微笑][微笑]     0.85   \n3               3                     [战双帕弥什_诶嘿][战双帕弥什_诶嘿][战双帕弥什_诶嘿]     1.00   \n4               4                                                永动姬     0.47   \n...           ...                                                ...      ...   \n39995       39995                      喜迎芬芳一周年，一路相伴张大仙（燃向）av28566658     0.95   \n39996       39996  另外说一下，视频后面的寒假作业，是仙友自制的688村委会文件，而不是现实生活中的寒假作业啦～...     0.91   \n39997       39997  图片来源于微博https://m.weibo.cn/1791027450/432633668...     0.97   \n39998       39998                                          换碟前bgm是什么     0.47   \n39999       39999                               我怀疑腾讯的员工都快过劳死，每天都在更新     0.01   \n\n       sentiment  \n0              7  \n1             10  \n2              9  \n3             10  \n4              5  \n...          ...  \n39995         10  \n39996         10  \n39997         10  \n39998          5  \n39999          1  \n\n[40000 rows x 4 columns]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 利用SnowNLP进行情绪打分，小于0.5属于差评，大于0.5属于好评\n",
    "def get(s):\n",
    "    if s>=0 and s<=0.1:\n",
    "        return 1\n",
    "    elif s>0.1 and s<=0.2:\n",
    "        return 2\n",
    "    elif s>0.2 and s<=0.3:\n",
    "        return 3\n",
    "    elif s>0.3 and s<=0.4:\n",
    "        return 4\n",
    "    elif s>0.4 and s<=0.5:\n",
    "        return 5\n",
    "    elif s>0.5 and s<=0.6:\n",
    "        return 6\n",
    "    elif s>0.6 and s<=0.7:\n",
    "        return 7\n",
    "    elif s>0.7 and s<=0.8:\n",
    "        return 8\n",
    "    elif s>0.8 and s<=0.9:\n",
    "        return 9\n",
    "    elif s>0.9 and s<=1:\n",
    "        return 10\n",
    "    \n",
    "\n",
    "def make_label(df):\n",
    "    df[\"sentiment\"] = df[\"SnowNLP\"].apply(get)\n",
    "make_label(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel('评论表.xlsx') # 导出到excel表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征、标签分开赋值\n",
    "X = data[[\"comment\"]]   \n",
    "y = data.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Building prefix dict from the default dictionary ...\nLoading model from cache C:\\Users\\Public\\Documents\\Wondershare\\CreatorTemp\\jieba.cache\nLoading model cost 1.156 seconds.\nPrefix dict has been built succesfully.\nE:\\ANACONDA\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \n"
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment</th>\n      <th>cuted_comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>达芬奇的恶魔（一语双关）</td>\n      <td>达芬奇 的 恶魔 （ 一语双关 ）</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>@战双帕弥什</td>\n      <td>@ 战双 帕弥什</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>我狂掉达尔文[微笑][微笑][微笑]</td>\n      <td>我狂 掉 达尔文 [ 微笑 ] [ 微笑 ] [ 微笑 ]</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>[战双帕弥什_诶嘿][战双帕弥什_诶嘿][战双帕弥什_诶嘿]</td>\n      <td>[ 战双 帕弥什 _ 诶 嘿 ] [ 战双 帕弥什 _ 诶 嘿 ] [ 战双 帕弥什 _ 诶...</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>永动姬</td>\n      <td>永动姬</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>手残福音。</td>\n      <td>手残 福音 。</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>不符合预期警告</td>\n      <td>不 符合 预期 警告</td>\n    </tr>\n    <tr>\n      <td>97</td>\n      <td>修复了</td>\n      <td>修复 了</td>\n    </tr>\n    <tr>\n      <td>98</td>\n      <td>好狠</td>\n      <td>好狠</td>\n    </tr>\n    <tr>\n      <td>99</td>\n      <td>我靠 手残失智玩家看了惊呆</td>\n      <td>我 靠   手残 失智 玩家 看 了 惊呆</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 2 columns</p>\n</div>",
      "text/plain": "                           comment  \\\n0                     达芬奇的恶魔（一语双关）   \n1                           @战双帕弥什   \n2               我狂掉达尔文[微笑][微笑][微笑]   \n3   [战双帕弥什_诶嘿][战双帕弥什_诶嘿][战双帕弥什_诶嘿]   \n4                              永动姬   \n..                             ...   \n95                           手残福音。   \n96                         不符合预期警告   \n97                             修复了   \n98                              好狠   \n99                   我靠 手残失智玩家看了惊呆   \n\n                                        cuted_comment  \n0                                   达芬奇 的 恶魔 （ 一语双关 ）  \n1                                            @ 战双 帕弥什  \n2                       我狂 掉 达尔文 [ 微笑 ] [ 微笑 ] [ 微笑 ]  \n3   [ 战双 帕弥什 _ 诶 嘿 ] [ 战双 帕弥什 _ 诶 嘿 ] [ 战双 帕弥什 _ 诶...  \n4                                                 永动姬  \n..                                                ...  \n95                                            手残 福音 。  \n96                                         不 符合 预期 警告  \n97                                               修复 了  \n98                                                 好狠  \n99                              我 靠   手残 失智 玩家 看 了 惊呆  \n\n[100 rows x 2 columns]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 利用jieba库，把每一个评价，进行分词。\n",
    "import jieba\n",
    "def chinese_word_cut(mytext):\n",
    "    return \" \".join(jieba.cut(mytext))\n",
    "\n",
    "X[\"cuted_comment\"] = X.comment.apply(chinese_word_cut)\n",
    "X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 接下来要将一团的数据，拆分成训练数据集、测试数据集 \n",
    "# 从sklearn.model_selection导入数据拆分函数train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(75, 2)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看数据集形状：\n",
    "X_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>comment</th>\n      <th>cuted_comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>35</td>\n      <td>[战双帕弥什_哼][战双帕弥什_哼][战双帕弥什_哼][战双帕弥什_哼][战双帕弥什_哼]</td>\n      <td>[ 战双 帕弥什 _ 哼 ] [ 战双 帕弥什 _ 哼 ] [ 战双 帕弥什 _ 哼 ] [...</td>\n    </tr>\n    <tr>\n      <td>94</td>\n      <td>快乐</td>\n      <td>快乐</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>[战双帕弥什_痴汉]</td>\n      <td>[ 战双 帕弥什 _ 痴汉 ]</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>意识十双人组：警察同志，那天我们就和往常一样，站在新手村出口处准备迫害新人，然后来了一个人，...</td>\n      <td>意识 十 双人 组 ： 警察 同志 ， 那天 我们 就 和 往常 一样 ， 站 在 新手村 ...</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>[无语]还没刷齐意识的只能先看看</td>\n      <td>[ 无 语 ] 还 没刷齐 意识 的 只能 先 看看</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>召唤师丽芙[吃瓜]</td>\n      <td>召唤师 丽 芙 [ 吃瓜 ]</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>意识掉的太乱了，六星意识分的话那个好点，换那个好点啊[战双帕弥什_肥宅]</td>\n      <td>意识 掉 的 太乱 了 ， 六星 意识 分 的话 那个 好点 ， 换 那个 好点 啊 [ 战...</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>我说怎么半夜修bug</td>\n      <td>我 说 怎么 半夜 修 bug</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>但是这个观赏高过伤害吧。。总感觉对于战区这种拼速度的 好像意义不是很大。。主要在于打有难度的...</td>\n      <td>但是 这个 观赏 高过 伤害 吧 。 。 总 感觉 对于 战区 这种 拼 速度 的   好像...</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>[战双帕弥什_喷了]</td>\n      <td>[ 战双 帕弥什 _ 喷 了 ]</td>\n    </tr>\n  </tbody>\n</table>\n<p>75 rows × 2 columns</p>\n</div>",
      "text/plain": "                                              comment  \\\n35      [战双帕弥什_哼][战双帕弥什_哼][战双帕弥什_哼][战双帕弥什_哼][战双帕弥什_哼]   \n94                                                 快乐   \n27                                         [战双帕弥什_痴汉]   \n46  意识十双人组：警察同志，那天我们就和往常一样，站在新手村出口处准备迫害新人，然后来了一个人，...   \n38                                   [无语]还没刷齐意识的只能先看看   \n..                                                ...   \n75                                          召唤师丽芙[吃瓜]   \n9                意识掉的太乱了，六星意识分的话那个好点，换那个好点啊[战双帕弥什_肥宅]   \n72                                         我说怎么半夜修bug   \n12  但是这个观赏高过伤害吧。。总感觉对于战区这种拼速度的 好像意义不是很大。。主要在于打有难度的...   \n37                                         [战双帕弥什_喷了]   \n\n                                        cuted_comment  \n35  [ 战双 帕弥什 _ 哼 ] [ 战双 帕弥什 _ 哼 ] [ 战双 帕弥什 _ 哼 ] [...  \n94                                                 快乐  \n27                                    [ 战双 帕弥什 _ 痴汉 ]  \n46  意识 十 双人 组 ： 警察 同志 ， 那天 我们 就 和 往常 一样 ， 站 在 新手村 ...  \n38                         [ 无 语 ] 还 没刷齐 意识 的 只能 先 看看  \n..                                                ...  \n75                                     召唤师 丽 芙 [ 吃瓜 ]  \n9   意识 掉 的 太乱 了 ， 六星 意识 分 的话 那个 好点 ， 换 那个 好点 啊 [ 战...  \n72                                    我 说 怎么 半夜 修 bug  \n12  但是 这个 观赏 高过 伤害 吧 。 。 总 感觉 对于 战区 这种 拼 速度 的   好像...  \n37                                   [ 战双 帕弥什 _ 喷 了 ]  \n\n[75 rows x 2 columns]"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用停用词表，创建停用词函数，创建为列表，\n",
    "def get_custom_stopword(stop_word_file):\n",
    "    with open(stop_word_file) as f:\n",
    "        stop_word = f.read()\n",
    "        \n",
    "    stop_word_list = stop_word.split(\"/n\")\n",
    "    custom_stopword = [i for i in stop_word_list]\n",
    "    return custom_stopword\n",
    "\n",
    "stopwords = get_custom_stopword(r\"F:\\python\\代码\\B站\\情感分析\\stop_words.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入 CountVectorizer函数，将中文词语向量化 ，使用默认参数即可\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from pandas import DataFrame\n",
    "vect = CountVectorizer()\n",
    "term_matrix = DataFrame(vect.fit_transform(X_train.cuted_comment).toarray(), columns=vect.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>16</th>\n      <th>1w</th>\n      <th>21e</th>\n      <th>2233</th>\n      <th>868854317</th>\n      <th>buff</th>\n      <th>bug</th>\n      <th>cd</th>\n      <th>doge</th>\n      <th>end</th>\n      <th>...</th>\n      <th>那天</th>\n      <th>重新</th>\n      <th>问题</th>\n      <th>难度</th>\n      <th>需要</th>\n      <th>音娘</th>\n      <th>预期</th>\n      <th>频率</th>\n      <th>马拉松</th>\n      <th>高过</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>75 rows × 265 columns</p>\n</div>",
      "text/plain": "    16  1w  21e  2233  868854317  buff  bug  cd  doge  end  ...  那天  重新  问题  \\\n0    0   0    0     0          0     0    0   0     0    0  ...   0   0   0   \n1    0   0    0     0          0     0    0   0     0    0  ...   0   0   0   \n2    0   0    0     0          0     0    0   0     0    0  ...   0   0   0   \n3    0   0    0     0          0     0    0   0     0    0  ...   1   0   0   \n4    0   0    0     0          0     0    0   0     0    0  ...   0   0   0   \n..  ..  ..  ...   ...        ...   ...  ...  ..   ...  ...  ...  ..  ..  ..   \n70   0   0    0     0          0     0    0   0     0    0  ...   0   0   0   \n71   0   0    0     0          0     0    0   0     0    0  ...   0   0   0   \n72   0   0    0     0          0     0    1   0     0    0  ...   0   0   0   \n73   0   0    0     0          0     0    0   0     0    0  ...   0   0   0   \n74   0   0    0     0          0     0    0   0     0    0  ...   0   0   0   \n\n    难度  需要  音娘  预期  频率  马拉松  高过  \n0    0   0   0   0   0    0   0  \n1    0   0   0   0   0    0   0  \n2    0   0   0   0   0    0   0  \n3    0   0   0   0   0    0   0  \n4    0   0   0   0   0    0   0  \n..  ..  ..  ..  ..  ..  ...  ..  \n70   0   0   0   0   0    0   0  \n71   0   0   0   0   0    0   0  \n72   0   0   0   0   0    0   0  \n73   1   0   0   0   0    0   1  \n74   0   0   0   0   0    0   0  \n\n[75 rows x 265 columns]"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_matrix #测试数据集为下图所示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(75, 265)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_matrix.shape #向量化后的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_df = 0.8 # 在超过这一比例的文档中出现的关键词（过于平凡），去除掉。\n",
    "min_df = 2 # 在低于这一数量的文档中出现的关键词（过于独特），去除掉。\n",
    "\n",
    "vect = CountVectorizer(max_df = max_df,\n",
    "                       min_df = min_df,\n",
    "                       token_pattern=u'(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b',\n",
    "                       stop_words=frozenset(stopwords))\n",
    "\n",
    "term_matrix = DataFrame(vect.fit_transform(X_train.cuted_comment).toarray(), columns=vect.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(75, 20)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_matrix.shape # 通过以上操作，去除很多无关数据，现在的数据集维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "\n",
    "X_train_vect = vect.fit_transform(X_train)\n",
    "nb.fit(X_train_vect, y_train)\n",
    "train_score = nb.score(X_train_vect, y_train)\n",
    "print(train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.885"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "\n",
    "X_train_vect = vect.fit_transform(X_train)\n",
    "nb.fit(X_train_vect, y_train)\n",
    "train_score = nb.score(X_train_vect, y_train)\n",
    "print(train_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_vec = vect.transform(X)\n",
    "sentime_result = nb.predict(X_vec)\n",
    "data['sentiment'] = sentime_result"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}